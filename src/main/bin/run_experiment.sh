#!/bin/bash
input_checks() {
	if [ "$1" == "-h" ]; then
			echo "Usage: $(basename "${0}") {BASE_PATH} {EXPERIMENT NUMBER - can be e1, e2 or e3} {NUMBER OF PARTITIONS}"
			exit 0
	fi

	if [ $# -lt 3 ]
	then
			echo "Missing Operand"
			echo "Run $(basename "${0}") -h for usage"
			exit 0
	fi

	echo "Your Input :-"
	echo "BASE_PATH - ${1}"
	echo "EXPERIMENT - ${2}"
	echo "NUMBER OF PARTITIONS - ${3}"
}

find_script_home() {
  pushd . > /dev/null
  SCRIPT_DIRECTORY="${BASH_SOURCE[0]}";
  while [ -h "${SCRIPT_DIRECTORY}" ];
  do
    cd "$(dirname "${SCRIPT_DIRECTORY}")" || exit
    SCRIPT_DIRECTORY="$(readlink "$(basename "${SCRIPT_DIRECTORY}")")";
  done
  cd "$(dirname "${SCRIPT_DIRECTORY}")" > /dev/null || exit
  SCRIPT_DIRECTORY="$(pwd)";
  popd  > /dev/null || exit
  APP_HOME="$(dirname "${SCRIPT_DIRECTORY}")"
}

main() {

	find_script_home
  input_checks "${@}"

	BASE_PATH=$1
	EXPERIMENT=$2
	PARTITIONS=$3

	# spark command for running an experiment
	spark-submit \
	--class edu.asu.sparkpartitioning.Main \
	--master spark://172.31.19.91:7077 \
	--deploy-mode client \
	"${APP_HOME}"/lib/Spark-Partitioning-0.1-SNAPSHOT.jar \
	hdfs://172.31.19.91:9000"${BASE_PATH}" hdfs://172.31.19.91:9000/spark/applicationHistory \
	"${PARTITIONS}" "${EXPERIMENT}"

	#remove temp data generated by the experiment from the dfs
	hdfs dfs -rm -r "${BASE_PATH}"/"${EXPERIMENT}"/*

}

main "${@}"